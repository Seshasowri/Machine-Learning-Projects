#!/usr/bin/env python
# coding: utf-8

# <h2>Project 2: The Perceptron</h2>
# 
# 
# <!--announcements-->
# <blockquote>
#     <center>
#     <img src="perceptron.png" width="200px" />
#     </center>
#       <p><cite><center>"What, we asked, wasn't the Perceptron capable of?"<br>
#       Rival, The New Yorker, December 6, 1958 P. 44</center>
#       </cite></p>
# </blockquote>
# 
# <h3>Introduction</h3>
# <!--AÃ°albrandr-->
# 
# <p>In this project, you will implement a simple Perceptron classifier to classify digits (or anything else).</p>
# 
# <strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.
# 
# Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.
# 
# <p><strong>Evaluation:</strong> Your code will be autograded for technical
# correctness. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.
# 
# <p><strong>Academic Dishonesty:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.
# 
# <p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href="https://piazza.com/class/icxgflcnpra3ko">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.  
# 

# <p><strong>Python initialization:</strong> Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>

# In[82]:


#<GRADED>
import numpy as np
from matplotlib import *
#matplotlib.use('PDF')
from pylab import *
#</GRADED>
import sys
import matplotlib.pyplot as plt
import time

# add p02 folder
sys.path.insert(0, './p02/')

get_ipython().run_line_magic('matplotlib', 'notebook')
print('You\'re running python %s' % sys.version.split(' ')[0])


# <h3> The Perceptron <b>(95 points)</b> </h3>
# 
# <p>The perceptron is a basic linear classifier. The following questions will ask you to finish these functions in a pre-defined order. Unless specified otherwise, do not use loops.<br></p>
# 
# <p>(a) Implement the process of updating the weight vector in the following function.
# 
# 
# 

# In[83]:


#<GRADED>
def perceptronUpdate(x,y,w):
    """
    function w=perceptronUpdate(x,y,w);
    
    Implementation of Perceptron weights updating
    Input:
    x : input vector of d dimensions (d)
    y : corresponding label (-1 or +1)
    w : weight vector of d dimensions
    
    Output:
    w : weight vector after updating (d)
    """
    assert(y in {-1,1})
    assert(len(w.shape)==1), "At the update w must be a vector not a matrix (try w=w.flatten())"
    assert(len(x.shape)==1), "At the update x must be a vector not a matrix (try x=x.flatten())"
    
    ## fill in code ...
    cond = y * np.dot(x,w)
    if ( cond <= 0 ):
        w = w + y*x
        
    ## ... until here
    return w.flatten()
#</GRADED>


# In[84]:


# test the update code:
x=rand(5) # random weight vector
w=rand(5) # random feature vector
y=-1 # random label
wnew=perceptronUpdate(x,y,w.copy()) # do a perceptron update
assert(norm(wnew-w+x)<1e-10), "perceptronUpdate didn't pass the test : (" # if correct, this should return 0
print("Looks like you passed the update test : )")


# <p>(b) Implement function <b><code>perceptron</code></b>. This should contain a loop that calls 
# <b><code>perceptronUpdate</code></b>
#  until it converges or the maximum iteration count, 100, has been reached.
#  Make sure you randomize the order of the training data on each iteration. </p>

# In[85]:


#<GRADED>
def perceptron(xs,ys):
    """
    function w=perceptron(xs,ys);
    
    Implementation of a Perceptron classifier
    Input:
    xs : n input vectors of d dimensions (nxd)
    ys : n labels (-1 or +1)
    
    Output:
    w : weight vector (1xd)
    b : bias term
    """

    assert(len(xs.shape)==2), "The first input to Perceptron must be a _matrix_ of row input vecdtors."
    assert(len(ys.shape)==1), "The second input to Perceptron must be a _vector_ of n labels (try ys.flatten())."
        
    n, d = xs.shape     # so we have n input vectors, of d dimensions each
    
    ## fill in code ...
       
    #identifying the index and shuffling them
    indexArray = np.arange(n)
    np.random.shuffle(indexArray)
    
    #add dimension to weight; initialize counter;
    ws = np.zeros(d + 1)
    counter = 0
    flag = 0
   
    # introducing extra dimension of 1s in x input
    ad = np.zeros((n,1))
    
    j = 0
    while (j < n) :
        ad[j,0] = 1
        j = j+1
    xsn = np.append(xs, ad, axis = 1)
    
    
    # calculating the weight and bias and counting the iterations as per requirement
    while (flag == 0):
        for i in indexArray:
            old_w = ws
            ws = perceptronUpdate(xsn[i, :],ys[i], ws)
            if np.all(old_w != ws):
                counter = counter + 1        
        b = ws[-1]   
        w = ws[:d]
    
        if (all(np.sign(ys*(xs.dot(w)+b))==1.0)):
            flag = flag + 1
            break
        elif (counter >=100):
            flag = flag + 1
            break

    ## ... until here
    return (w,b)

#</GRADED>


# <p> You can use the following script to test your code and visualize your perceptron on linearly separable data in 2 dimensions. Your classifier should find a separating hyperplane on such data.   </p>

# In[86]:


# number of input vectors
N = 100

# generate random (linarly separable) data
xs = np.random.rand(N, 2)*10-5

# defining random hyperplane
w0 = np.random.rand(2)
b0 = rand()*2-1;

# assigning labels +1, -1 labels depending on what side of the plane they lie on
ys = np.sign(xs.dot(w0)+b0)

# call perceptron to find w from data
w,b = perceptron(xs.copy(),ys.copy())

# test if all points are classified correctly
assert (all(np.sign(ys*(xs.dot(w)+b))==1.0))  # yw'x should be +1.0 for every input
print("Looks like you passed the Perceptron test! :o)")

# we can make a pretty visualizxation
from helperfunctions import visboundary
visboundary(w,b,xs,ys)


# In[87]:


ion()
def onclick(event):
    global w,b,ldata,ax,line,xydata

    pos=np.array([[event.xdata],[event.ydata]])
    if event.key == 'shift': # add positive point
        color='or'
        label=1
    else: # add negative point
        color='ob'
        label=-1    
    ax.plot(pos[0],pos[1],color)
    ldata.append(label);
    xydata=np.vstack((xydata,pos.T))
    
    # call Perceptron function
    w,b=perceptron(xydata,np.array(ldata).flatten())

    # draw decision boundary
    q=-b/(w**2).sum() *w;
    if line==None:
        line, = ax.plot([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]],'b--')
    else:
        line.set_data([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]])
        


xydata=rand(0,2)
ldata=[]
w=zeros(2)
b=0
line=None

fig = plt.figure()
ax = fig.add_subplot(111)
plt.xlim(0,1)
plt.ylim(0,1)
cid = fig.canvas.mpl_connect('button_press_event', onclick)
title('Use shift-click to add negative points.')


# <p>(c) 
# 	Implement 
# <b><code>classifyLinear</code></b>
#  that applies the weight vector and bias to the input vector. (The bias is an optional parameter. If it is not passed in, assume it is zero.) Make sure that the predictions returned are either 1 or -1.</p> 
# 
# 

# In[88]:


#<GRADED>
def classifyLinear(xs,w,b):
    """
    function preds=classifyLinear(xs,w,b)
    
    Make predictions with a linear classifier
    Input:
    xs : n input vectors of d dimensions (nxd) [could also be a single vector of d dimensions]
    w : weight vector of dimensionality d
    b : bias (scalar)
    
    Output:
    preds: predictions (1xn)
    """    
    w = w.flatten()    
    predictions=np.zeros(xs.shape[0])
    ## fill in code ...
     #introducing default value of bias
    if (b == None):
        bs = 0    
    else:
        bs = b
        
    # checking number of dimensions in vector xs    
    assert(len(xs.shape)==2), "The first input to Perceptron must be a _matrix_ of row input vecdtors."
        
    n, d = xs.shape     # so we have n input vectors, of d dimensions each
    
    #predicting the values as per inputs
    j = 0
    while (j < n):
        term = np.dot(w, xs[j, :]) + bs
        
        if (term > 0):
            predictions[j] = +1    
        else:
            predictions[j] = -1
        
        j = j + 1
        
    ## ... until here
    return predictions
#</GRADED>


# In[89]:


# test classifyLinear code:
xs=rand(1000,2)-0.5 # draw random data 
w0=np.array([0.5,-0.3]) # define a random hyperplane 
b0=-0.1 # with bias -0.1
ys=np.sign(xs.dot(w0)+b0) # assign labels according to this hyperplane (so you know it is linearly separable)
assert (all(np.sign(ys*classifyLinear(xs,w0,b0))==1.0))  # the original hyperplane (w0,b0) should classify all correctly
print("Looks like you passed the classifyLinear test! :o)")


# <h3>Competition <b>(5 points)</b></h3>
# 
# <p>The competition for this assignment is to achieve the highest accuracy on the hidden test set (randomly sampled) using the perceptron algorithm you implemented above. You will have access to a training, and a validation set but not the actual test set.
#     
# You will receive full points on this section as long as you beat a baseline which we have implemented.
# 
# We will have a leaderboard once the assignement due date has passed showing how well you did on the test set.</p>
# 
# <p>The competition for this assignment is split into two components you can modify:</p>
# 
# <ol>
# <li><b>Feature Extraction</b>:
# Modify the function <code>extractfeaturescomp</code>.
# This function takes in a file path <code>path</code> and
# a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.
# The autograder will pass in a file path pointing to a file that contains an email,
# and set <code>B</code> = <code>feature_dimension</code>.
# We provide <code>extractfeaturesnaive</code> as an example.
# </li>
# <li><b>Model Training</b>:
# Modify the function <code>trainspamfiltercomp</code>.
# This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and
# should output a weight vector <code>w</code> for linear classification. <b>You must use the perceptron algorithm implemented above </b> although you are free to tweak the parameters such as the number of iterations.
# We provide an initial implementation of a random classifier.
# </li>
# </ol>
# 
# <p>Your model will be trained on the following dataset (loaded by <code>loadspamdata</code>), but we will test its accuracy on a secret dataset of emails.</p>

# In[90]:


# tokenize the email and hashes the symbols into a vector
def extractfeaturesnaive(path, B):
    with open(path, 'r') as femail:
        # initialize all-zeros feature vector
        v = np.zeros(B)
        email = femail.read()
        # breaks for non-ascii characters
#         print(email)
        tokens = email.split()
        for token in tokens:
            v[hash(token) % B] = 1
    return v

def loadspamdata(extractfeatures, B=256, path="../resource/lib/public/new_train_data/"):
    '''
    INPUT:
    extractfeatures : function to extract features
    B               : dimensionality of feature space
    path            : the path of folder to be processed
    
    OUTPUT:
    X, Y
    '''
    if path[-1] != '/':
        path += '/'
    
    with open(path + 'index', 'r') as f:
        allemails = [x for x in f.read().split('\n') if ' ' in x]
    
    xs = np.zeros((len(allemails), B))
    ys = np.zeros(len(allemails))
    for i, line in enumerate(allemails):
#         print(i, line)
        label, filename = line.split(' ')
        # make labels +1 for "spam" and -1 for "ham"
        ys[i] = (label == 'spam') * 2 - 1
#         if (i == 8):
#             print(label)
        xs[i, :] = extractfeatures(path + filename, B)
    print('Loaded %d input emails.' % len(ys))
    return xs, ys

X,Y = loadspamdata(extractfeaturesnaive)
X.shape


# This is your training set. To do proper model selection and avoid overfitting, you should split it off into a validation set. Here's one implementation but feel free to <b> try other methods including k-fold cross validation </b>.

# In[91]:


#<GRADED>

def validation_split(X, Y):
    # Split data into training and validation
    n, d = X.shape
    cutoff = int(np.ceil(0.8 * n))
    # indices of training samples
    xTr = X[:cutoff,:]
    yTr = Y[:cutoff]
    # indices of validation samples
    xTv = X[cutoff:,:]
    yTv = Y[cutoff:]

    ## fill in code ...
    ## ... until here
    
    return xTr, yTr, xTv, yTv

#</GRADED>

xTr, yTr, xTv, yTv = validation_split(X, Y)


# <p>This should generate a training data set <code>xTr</code>, <code>yTr</code> and a validation set <code>xTv</code>, <code>yTv</code> for you. </p>
# 
# <p>It is now time to implement your classifiers.</p>

# In[92]:


#<GRADED>
## do NOT change feature_dimension
feature_dimension = 256

def extractfeaturescomp(path, B):
    '''
    INPUT:
    path : file path of email
    B    : dimensionality of feature vector
    
    OUTPUTS:
    x    : B dimensional vector
    '''
    ## naive implementation (same as extractfeaturesnaive(path, B))
    x = np.zeros(B)
    with open(path, 'r') as femail:
        email = femail.read()
        tokens = email.split()
        for token in tokens:
            x[hash(token) % B] = 1
            
    ## fill in code ...
    ## your implementation
    
    ## ... until here
    
    return x
    
#</GRADED>


# In[93]:


#<GRADED>

def perceptronComp(xs,ys):
    """
    function w=perceptron(xs,ys);
    
    Implementation of a Perceptron classifier
    Input:
    xs : n input vectors of d dimensions (nxd)
    ys : n labels (-1 or +1)
    
    Output:
    w : weight vector (1xd)
    b : bias term
    """

    assert(len(xs.shape)==2), "The first input to Perceptron must be a _matrix_ of row input vecdtors."
    assert(len(ys.shape)==1), "The second input to Perceptron must be a _vector_ of n labels (try ys.flatten())."
        
    n, d = xs.shape     # so we have n input vectors, of d dimensions each
    
    ## fill in code ...
       
    #identifying the index and shuffling them
    indexArray = np.arange(n)
    np.random.shuffle(indexArray)
    
    #add dimension to weight; initialize counter;
    ws = np.zeros(d + 1)
    counter = 0
    flag = 0
   
    # introducing extra dimension of 1s in x input
    ad = np.zeros((n,1))
    
    j = 0
    while (j < n) :
        ad[j,0] = 1
        j = j+1
    xsn = np.append(xs, ad, axis = 1)
    
    
    # calculating the weight and bias and counting the iterations as per requirement
    while (flag == 0):
        for i in indexArray:
            old_w = ws
            ws = perceptronUpdate(xsn[i, :],ys[i], ws)
            if np.all(old_w != ws):
                counter = counter + 1        
        b = ws[-1]   
        w = ws[:d]
    
        if (all(np.sign(ys*(xs.dot(w)+b))==1.0)):
            flag = flag + 1
            break
        elif (counter >=1000):
            flag = flag + 1
            break

    ## ... until here
    return (w,b)

def trainspamfiltercomp(xTr, yTr):
    '''
    INPUT:
    xTr : nxd dimensional matrix (each row is an input vector)
    yTr : d   dimensional vector (each entry is a label)
    
    OUTPUTS:
    w : d dimensional vector for linear classification
    '''
    w = np.random.rand(np.shape(xTr)[1])
    b = np.random.rand()
    
    ## fill in code ...
    w, b = perceptronComp(xTr.copy(), yTr.copy())
#     print(w, b)
    ## ... until here    
    
    return w,b
#</GRADED>


# In[94]:


## Evaluate the performance on your validation set here
X,Y = loadspamdata(extractfeaturesnaive)
xTr, yTr, xTv, yTv = validation_split(X, Y)
w, b = trainspamfiltercomp(xTr.copy(), yTr.copy())

pred = classifyLinear(xTv, w, b)
# print (pred)
# print (yTv)
correct = 0
for i in range(len(pred)):
    if (pred[i] == yTv[i]):
        correct += 1
print ("performance is ", correct/len(pred))


# In[ ]:




